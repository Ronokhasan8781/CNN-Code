{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9037092,
          "sourceType": "datasetVersion",
          "datasetId": 5447573
        }
      ],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "TransUnet Mdel result",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronokhasan8781/CNN-Code/blob/main/TransUnet_Mdel_result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'hhhhhhhhhhhh:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5447573%2F9037092%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240726%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240726T064201Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0cec22393a3d5ae88beed522780068e912d44f4237fd745f2f43568ba208804caa16f3e0815ad8037d06f8b584d60fbfd9011a4a97c00944818b0dc79028d0dc10d01e3261654a2294c32a7889d70780c629257b604659af2af571a35dd492589c2276890a19911bde28beddf4b481d6f270f6e5851cb3cf47aa477a43fa0ab3facce47c1b4f3b5b19a4fb2315f585fd0296e0156a5f543b2d6831a8aded636ac8f01e5d8a3ba8184b46f2b21a603dc2fd202978b6ef870a2a0f3f4abb7625ad7daa7e15ce617d86fc9f54d4b22afaed38ed78b260240f8c141dc531f89d468f719a972a2b2a35f495c08289ea7ec09ffeb043c5b8f9584795f071d8d2b29498'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ZpD4etm9xncy"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-26T05:46:18.873296Z",
          "iopub.execute_input": "2024-07-26T05:46:18.873659Z",
          "iopub.status.idle": "2024-07-26T05:46:19.236228Z",
          "shell.execute_reply.started": "2024-07-26T05:46:18.87363Z",
          "shell.execute_reply": "2024-07-26T05:46:19.235403Z"
        },
        "trusted": true,
        "id": "tMRmcXhyxnc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the image dimensions and channels\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "# Load the file lists\n",
        "Original = os.listdir('/kaggle/input/hhhhhhhhhhhh/images')\n",
        "Masks = os.listdir('/kaggle/input/hhhhhhhhhhhh/masks')\n",
        "\n",
        "# Printing the number of images and masks\n",
        "print(len(Original))\n",
        "print(len(Masks))\n",
        "\n",
        "# Initialize the arrays with the correct shapes\n",
        "X = np.zeros((len(Original), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
        "y = np.zeros((len(Masks), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.int32)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming Original and Masks are lists of filenames\n",
        "for file in Original:\n",
        "    try:\n",
        "        n = Original.index(file)\n",
        "        path1 = os.path.join('/kaggle/input/hhhhhhhhhhhh/images', file)\n",
        "        print(path1)\n",
        "        img = cv2.imread(path1)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        X[n] = img / 255\n",
        "\n",
        "        file2 = Masks[n]\n",
        "        path2 = os.path.join('/kaggle/input/hhhhhhhhhhhh/masks', file2)\n",
        "        mask = cv2.imread(path2, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, (256, 256))\n",
        "        mask = tf.one_hot(mask, 1, dtype=tf.int32)\n",
        "        y[n] = mask\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(path1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T05:46:19.237719Z",
          "iopub.execute_input": "2024-07-26T05:46:19.238015Z",
          "iopub.status.idle": "2024-07-26T05:47:06.583119Z",
          "shell.execute_reply.started": "2024-07-26T05:46:19.23799Z",
          "shell.execute_reply": "2024-07-26T05:47:06.582169Z"
        },
        "trusted": true,
        "id": "XHBLu6sYxnc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, BatchNormalization, Activation, LayerNormalization, Reshape, Permute, Multiply, Add, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming you have X and y defined as your input images and masks\n",
        "# Replace with your actual data loading and preprocessing\n",
        "# X and y should be numpy arrays with shape (num_samples, height, width, channels)\n",
        "# Example:\n",
        "# X = np.load('images.npy')\n",
        "# y = np.load('masks.npy')\n",
        "\n",
        "# Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=35,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define callbacks for learning rate scheduling, early stopping, and model checkpointing\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# Define data generators\n",
        "batch_size = 16\n",
        "train_generator = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "test_generator = datagen.flow(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = Permute([2, 1])(x)\n",
        "    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(x, x)\n",
        "    x = Add()([attention_out, inputs])\n",
        "\n",
        "    # Feed Forward Part\n",
        "    y = LayerNormalization(epsilon=1e-6)(x)\n",
        "    y = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = Conv1D(filters=inputs.shape[-1], kernel_size=1)(y)\n",
        "    return Add()([x, y])\n",
        "\n",
        "\n",
        "\n",
        "def trans_unet_attention(input_size=(256, 256, 3), head_size=256, num_heads=4, ff_dim=1024, num_transformer_blocks=4):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # U-Net Contracting Path\n",
        "    c1 = conv_block(inputs, 16)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = conv_block(p1, 32)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = conv_block(p2, 64)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = conv_block(p3, 128)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    c5 = conv_block(p4, 256)\n",
        "\n",
        "    # Transformer Encoder-Decoder Blocks\n",
        "    x = Reshape((-1, c5.shape[1]*c5.shape[2]))(c5)\n",
        "    mask = tf.linalg.band_part(tf.ones((x.shape[1], x.shape[1])), -1, 0)\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim)\n",
        "    x = Reshape((c5.shape[1], c5.shape[2], -1))(x)\n",
        "\n",
        "\n",
        "    # U-Net Expanding Path with Attention\n",
        "    u4 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "    u4 = concatenate([u4, c4])\n",
        "    u4 = conv_block(u4, 128)\n",
        "\n",
        "    u3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u4)\n",
        "    u3 = concatenate([u3, c3])\n",
        "    u3 = conv_block(u3, 64)\n",
        "\n",
        "    u2 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(u3)\n",
        "    u2 = concatenate([u2, c2])\n",
        "    u2 = conv_block(u2, 32)\n",
        "\n",
        "    u1 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(u2)\n",
        "    u1 = concatenate([u1, c1])\n",
        "    u1 = conv_block(u1, 16)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(u1)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = trans_unet_attention()\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(X_train) // batch_size,\n",
        "                    epochs=100,  # Increase epochs for further training\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps=len(X_test) // batch_size,\n",
        "                    callbacks=[reduce_lr, early_stopping, model_checkpoint])\n",
        "\n",
        "# Evaluate the model\n",
        "model = tf.keras.models.load_model('best_model.keras')  # Load the best model\n",
        "y_pred = model.predict(test_generator, steps=len(X_test) // batch_size + 1)\n",
        "y_pred = (y_pred > 0.5).astype(np.uint8)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T05:47:06.58453Z",
          "iopub.execute_input": "2024-07-26T05:47:06.585158Z",
          "iopub.status.idle": "2024-07-26T06:31:32.797744Z",
          "shell.execute_reply.started": "2024-07-26T05:47:06.585101Z",
          "shell.execute_reply": "2024-07-26T06:31:32.796869Z"
        },
        "trusted": true,
        "id": "TxE2uFObxnc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting training and validation accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print final accuracy and loss\n",
        "final_train_acc = acc[-1]\n",
        "final_val_acc = val_acc[-1]\n",
        "final_train_loss = loss[-1]\n",
        "final_val_loss = val_loss[-1]\n",
        "\n",
        "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")# Plot training and validation accuracy\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predictions and true labels to binary\n",
        "y_pred_binary = (y_pred > 0.5).astype(np.int32)\n",
        "y_test_binary = (y_test > 0.5).astype(np.int32)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_binary.flatten(), y_pred_binary.flatten())\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_binary.flatten(), y_pred_binary.flatten()))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test_binary.flatten(), y_pred.flatten())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample true labels and predicted labels (values between 0 and 1)\n",
        "y_true = np.random.randint(2, size=100)\n",
        "y_pred = np.random.rand(100)\n",
        "\n",
        "# Binarize predictions based on a threshold of 0.5\n",
        "y_pred_bin = (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_bin)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T06:31:32.800646Z",
          "iopub.execute_input": "2024-07-26T06:31:32.801064Z",
          "iopub.status.idle": "2024-07-26T06:32:17.194485Z",
          "shell.execute_reply.started": "2024-07-26T06:31:32.801029Z",
          "shell.execute_reply": "2024-07-26T06:32:17.193492Z"
        },
        "trusted": true,
        "id": "ivYpdQWwxnc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def compute_iou(y_true, y_pred, num_classes):\n",
        "    iou_scores = []\n",
        "\n",
        "    y_true = tf.argmax(y_true, axis=-1)\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        y_true_class = tf.cast(y_true == i, tf.float32)\n",
        "        y_pred_class = tf.cast(y_pred == i, tf.float32)\n",
        "\n",
        "        intersection = tf.reduce_sum(tf.cast(y_true_class * y_pred_class, tf.float32))\n",
        "        union = tf.reduce_sum(tf.cast(y_true_class + y_pred_class, tf.float32)) - intersection\n",
        "\n",
        "        iou = (intersection + 1e-10) / (union + 1e-10)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "    mean_iou = tf.reduce_mean(iou_scores)\n",
        "    return mean_iou\n",
        "\n",
        "# Example usage:\n",
        "y_true = np.random.randint(0, 2, (4, 256, 256, 1))\n",
        "y_pred = np.random.randint(0, 2, (4, 256, 256, 1))\n",
        "\n",
        "num_classes = 2\n",
        "iou_score = compute_iou(y_true, y_pred, num_classes)\n",
        "print(\"Mean IOU Score:\", iou_score)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T06:32:17.19561Z",
          "iopub.execute_input": "2024-07-26T06:32:17.195878Z",
          "iopub.status.idle": "2024-07-26T06:32:17.243456Z",
          "shell.execute_reply.started": "2024-07-26T06:32:17.195854Z",
          "shell.execute_reply": "2024-07-26T06:32:17.242565Z"
        },
        "trusted": true,
        "id": "DoXR7NzZxnc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Training Loss, Validation Loss, Training Accuracy\n",
        "training_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "training_accuracy = history.history['accuracy']\n",
        "validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "print(\"Training Loss:\", training_loss[-1])\n",
        "print(\"Validation Loss:\", validation_loss[-1])\n",
        "print(\"Training Accuracy:\", training_accuracy[-1])\n",
        "print(\"Validation Accuracy:\", validation_accuracy[-1])\n",
        "\n",
        "# Training Time\n",
        "start_time = time.time()\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(X_train) // batch_size,\n",
        "                    epochs=100,  # Adjust the number of epochs as needed\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps=len(X_test) // batch_size,\n",
        "                    callbacks=[reduce_lr, early_stopping, model_checkpoint])\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(\"Training Time:\", training_time)\n",
        "\n",
        "# Test Accuracy\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, steps=len(X_test) // batch_size)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T06:32:17.244531Z",
          "iopub.execute_input": "2024-07-26T06:32:17.244831Z",
          "iopub.status.idle": "2024-07-26T06:37:28.951151Z",
          "shell.execute_reply.started": "2024-07-26T06:32:17.244806Z",
          "shell.execute_reply": "2024-07-26T06:37:28.95014Z"
        },
        "trusted": true,
        "id": "iP4VBXVZxnc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Time: {:.2f} seconds\".format(training_time))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T06:37:28.952351Z",
          "iopub.execute_input": "2024-07-26T06:37:28.952642Z",
          "iopub.status.idle": "2024-07-26T06:37:28.95734Z",
          "shell.execute_reply.started": "2024-07-26T06:37:28.952616Z",
          "shell.execute_reply": "2024-07-26T06:37:28.9565Z"
        },
        "trusted": true,
        "id": "Gm31jm1lxnc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5A2aLXPxnc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zJU0G2Oxnc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bOstaDXxnc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2w2Abukxnc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEJYwRkrxnc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}