{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 2645886,
          "sourceType": "datasetVersion",
          "datasetId": 1608934
        }
      ],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Federated-learning to predict brain tumor",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronokhasan8781/CNN-Code/blob/main/94%25Federated_learning_to_predict_brain_tumor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'brain-tumor-mri-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1608934%2F2645886%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240925%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240925T194749Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dd4647ee53fcd81deebf499473155097593e350382ec8b8fe0933c51a43a79ced91dd94c5526439993455db31d439e44fc73ab52fbdf840c9a8363cb92d618527c4c5740da52261f9714b725760703fe13118198324016725e10a521386349f69a8a438a2b9a554374ba3506a5f5130b184a55bceaf8af1819c997f5ff90aa3df4cd678ebe8a6a30ce83ac6aeb9d5f658dd05a57e2715282434bb1520f5738d3bad8e0dc6ab3b554775bce9adb85a05373e389542f585a826ad4967d6f0049baf9b004b2a3240b81c6f91562abdaffbae93a67b500bceac1f25b9f4fb57755b995475005c6bf549d98a5464a883241612897890ee919a58c35b6bbdca944b2c9b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "N2m0AH--ucw7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# For Data Processing\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "# For ML Models\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.metrics import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# For Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Miscellaneous\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.409812Z",
          "iopub.execute_input": "2024-09-25T11:09:14.410177Z",
          "iopub.status.idle": "2024-09-25T11:09:14.417136Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.410147Z",
          "shell.execute_reply": "2024-09-25T11:09:14.416141Z"
        },
        "trusted": true,
        "id": "bIcd2LkXucxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/kaggle/input/brain-tumor-mri-dataset/Training/'\n",
        "test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing/'\n",
        "\n",
        "train_paths = []\n",
        "train_labels = []\n",
        "\n",
        "for label in os.listdir(train_dir):\n",
        "    for image in os.listdir(train_dir+label):\n",
        "        train_paths.append(train_dir+label+'/'+image)\n",
        "        train_labels.append(label)\n",
        "\n",
        "train_paths, train_labels = shuffle(train_paths, train_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.418763Z",
          "iopub.execute_input": "2024-09-25T11:09:14.419074Z",
          "iopub.status.idle": "2024-09-25T11:09:14.447152Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.419048Z",
          "shell.execute_reply": "2024-09-25T11:09:14.446362Z"
        },
        "trusted": true,
        "id": "JTFlKX8uucxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "colors = ['#F6F7C1', '#BEF0CB', '#D1FFF3', '#C1AEFC']\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.pie([len([x for x in train_labels if x=='pituitary']),\n",
        "         len([x for x in train_labels if x=='notumor']),\n",
        "         len([x for x in train_labels if x=='meningioma']),\n",
        "         len([x for x in train_labels if x=='glioma'])],\n",
        "        labels=['pituitary','notumor', 'meningioma', 'glioma'],\n",
        "        colors=colors, autopct='%.1f%%', explode=(0.015,0.015,0.015,0.015),\n",
        "        startangle=30);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.448973Z",
          "iopub.execute_input": "2024-09-25T11:09:14.449782Z",
          "iopub.status.idle": "2024-09-25T11:09:14.574603Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.449744Z",
          "shell.execute_reply": "2024-09-25T11:09:14.573525Z"
        },
        "trusted": true,
        "id": "GsCf5HIGucxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_paths = []\n",
        "test_labels = []\n",
        "\n",
        "for label in os.listdir(test_dir):\n",
        "    for image in os.listdir(test_dir+label):\n",
        "        test_paths.append(test_dir+label+'/'+image)\n",
        "        test_labels.append(label)\n",
        "\n",
        "test_paths, test_labels = shuffle(test_paths, test_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.576273Z",
          "iopub.execute_input": "2024-09-25T11:09:14.576645Z",
          "iopub.status.idle": "2024-09-25T11:09:14.587959Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.576597Z",
          "shell.execute_reply": "2024-09-25T11:09:14.586922Z"
        },
        "trusted": true,
        "id": "XTAkM6EuucxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "colors = ['#BEF0CB', '#C1AEFC']\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.pie([len(train_labels), len(test_labels)],\n",
        "        labels=['Train','Test'],\n",
        "        colors=colors, autopct='%.1f%%', explode=(0.05,0),\n",
        "        startangle=30);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.590837Z",
          "iopub.execute_input": "2024-09-25T11:09:14.591209Z",
          "iopub.status.idle": "2024-09-25T11:09:14.706978Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.591174Z",
          "shell.execute_reply": "2024-09-25T11:09:14.706053Z"
        },
        "trusted": true,
        "id": "1A40Gfv_ucxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(image):\n",
        "    image = Image.fromarray(np.uint8(image))\n",
        "    image = ImageEnhance.Brightness(image).enhance(random.uniform(0.8,1.2))\n",
        "    image = ImageEnhance.Contrast(image).enhance(random.uniform(0.8,1.2))\n",
        "    image = np.array(image)/255.0\n",
        "    return image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.708238Z",
          "iopub.execute_input": "2024-09-25T11:09:14.708975Z",
          "iopub.status.idle": "2024-09-25T11:09:14.715715Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.708939Z",
          "shell.execute_reply": "2024-09-25T11:09:14.714725Z"
        },
        "trusted": true,
        "id": "nlexXAnFucxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 128\n",
        "\n",
        "def open_images(paths):\n",
        "    '''\n",
        "    Given a list of paths to images, this function returns the images as arrays (after augmenting them)\n",
        "    '''\n",
        "    images = []\n",
        "    for path in paths:\n",
        "        image = load_img(path, target_size=(IMAGE_SIZE,IMAGE_SIZE))\n",
        "        image = augment_image(image)\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "images = open_images(train_paths[50:59])\n",
        "labels = train_labels[50:59]\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "for x in range(1, 9):\n",
        "    fig.add_subplot(2, 4, x)\n",
        "    plt.axis('off')\n",
        "    plt.title(labels[x])\n",
        "    plt.imshow(images[x])\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:14.717407Z",
          "iopub.execute_input": "2024-09-25T11:09:14.717749Z",
          "iopub.status.idle": "2024-09-25T11:09:15.848314Z",
          "shell.execute_reply.started": "2024-09-25T11:09:14.717717Z",
          "shell.execute_reply": "2024-09-25T11:09:15.847413Z"
        },
        "trusted": true,
        "id": "28-Ptl0OucxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = os.listdir(train_dir)\n",
        "\n",
        "def encode_label(labels):\n",
        "    encoded = []\n",
        "    for x in labels:\n",
        "        encoded.append(unique_labels.index(x))\n",
        "    return np.array(encoded)\n",
        "\n",
        "def decode_label(labels):\n",
        "    decoded = []\n",
        "    for x in labels:\n",
        "        decoded.append(unique_labels[x])\n",
        "    return np.array(decoded)\n",
        "\n",
        "def datagen(paths, labels, batch_size=12, epochs=1):\n",
        "    for _ in range(epochs):\n",
        "        for x in range(0, len(paths), batch_size):\n",
        "            batch_paths = paths[x:x+batch_size]\n",
        "            batch_images = open_images(batch_paths)\n",
        "            batch_labels = labels[x:x+batch_size]\n",
        "            batch_labels = encode_label(batch_labels)\n",
        "            yield batch_images, batch_labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:09:15.850485Z",
          "iopub.execute_input": "2024-09-25T11:09:15.850778Z",
          "iopub.status.idle": "2024-09-25T11:09:15.858518Z",
          "shell.execute_reply.started": "2024-09-25T11:09:15.850752Z",
          "shell.execute_reply": "2024-09-25T11:09:15.85754Z"
        },
        "trusted": true,
        "id": "35raB77GucxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Data Processing\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "# For ML Models\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.metrics import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# For Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Miscellaneous\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Directories\n",
        "train_dir = '/kaggle/input/brain-tumor-mri-dataset/Training/'\n",
        "test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing/'\n",
        "\n",
        "# Load training paths and labels\n",
        "train_paths = []\n",
        "train_labels = []\n",
        "\n",
        "for label in os.listdir(train_dir):\n",
        "    for image in os.listdir(train_dir + label):\n",
        "        train_paths.append(train_dir + label + '/' + image)\n",
        "        train_labels.append(label)\n",
        "\n",
        "train_paths, train_labels = shuffle(train_paths, train_labels)\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "colors = ['#F6F7C1', '#BEF0CB', '#D1FFF3', '#C1AEFC']\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.pie([len([x for x in train_labels if x == 'pituitary']),\n",
        "          len([x for x in train_labels if x == 'notumor']),\n",
        "          len([x for x in train_labels if x == 'meningioma']),\n",
        "          len([x for x in train_labels if x == 'glioma'])],\n",
        "        labels=['pituitary', 'notumor', 'meningioma', 'glioma'],\n",
        "        colors=colors, autopct='%.1f%%', explode=(0.015, 0.015, 0.015, 0.015),\n",
        "        startangle=30)\n",
        "\n",
        "# Load testing paths and labels\n",
        "test_paths = []\n",
        "test_labels = []\n",
        "\n",
        "for label in os.listdir(test_dir):\n",
        "    for image in os.listdir(test_dir + label):\n",
        "        test_paths.append(test_dir + label + '/' + image)\n",
        "        test_labels.append(label)\n",
        "\n",
        "test_paths, test_labels = shuffle(test_paths, test_labels)\n",
        "\n",
        "# Visualize training and testing data distribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "colors = ['#BEF0CB', '#C1AEFC']\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.pie([len(train_labels), len(test_labels)],\n",
        "        labels=['Train', 'Test'],\n",
        "        colors=colors, autopct='%.1f%%', explode=(0.05, 0),\n",
        "        startangle=30)\n",
        "\n",
        "# Image augmentation function\n",
        "def augment_image(image):\n",
        "    image = Image.fromarray(np.uint8(image))\n",
        "    image = ImageEnhance.Brightness(image).enhance(random.uniform(0.8, 1.2))\n",
        "    image = ImageEnhance.Contrast(image).enhance(random.uniform(0.8, 1.2))\n",
        "    image = np.array(image) / 255.0\n",
        "    return image\n",
        "\n",
        "IMAGE_SIZE = 128\n",
        "\n",
        "# Function to load images\n",
        "def open_images(paths):\n",
        "    images = []\n",
        "    for path in paths:\n",
        "        image = load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "        image = augment_image(image)\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Visualize some images\n",
        "images = open_images(train_paths[50:59])\n",
        "labels = train_labels[50:59]\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "for x in range(1, 9):\n",
        "    fig.add_subplot(2, 4, x)\n",
        "    plt.axis('off')\n",
        "    plt.title(labels[x])\n",
        "    plt.imshow(images[x])\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.show()\n",
        "\n",
        "# Get unique labels\n",
        "unique_labels = os.listdir(train_dir)\n",
        "\n",
        "# Label encoding and decoding functions\n",
        "def encode_label(labels):\n",
        "    encoded = []\n",
        "    for x in labels:\n",
        "        encoded.append(unique_labels.index(x))\n",
        "    return np.array(encoded)\n",
        "\n",
        "def decode_label(labels):\n",
        "    decoded = []\n",
        "    for x in labels:\n",
        "        decoded.append(unique_labels[x])\n",
        "    return np.array(decoded)\n",
        "\n",
        "# Data generator function\n",
        "def datagen(paths, labels, batch_size=12, epochs=1):\n",
        "    for _ in range(epochs):\n",
        "        for x in range(0, len(paths), batch_size):\n",
        "            batch_paths = paths[x:x + batch_size]\n",
        "            batch_images = open_images(batch_paths)\n",
        "            batch_labels = labels[x:x + batch_size]\n",
        "            batch_labels = encode_label(batch_labels)\n",
        "            yield batch_images, batch_labels\n",
        "\n",
        "# Load ResNet50 model\n",
        "base_model = ResNet50(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Set layers to non-trainable\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Set the last few blocks to trainable if desired\n",
        "for layer in base_model.layers[-20:]:  # Adjust the number of layers to fine-tune as needed\n",
        "    layer.trainable = True\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(unique_labels), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 45\n",
        "steps = int(len(train_paths) / batch_size)\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),\n",
        "                    epochs=epochs, steps_per_epoch=steps)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T11:28:06.217029Z",
          "iopub.execute_input": "2024-09-25T11:28:06.217956Z",
          "iopub.status.idle": "2024-09-25T11:54:38.022701Z",
          "shell.execute_reply.started": "2024-09-25T11:28:06.217915Z",
          "shell.execute_reply": "2024-09-25T11:54:38.021577Z"
        },
        "trusted": true,
        "id": "ztEgbczNucxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueJr2qqmucxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBhcAzd0ucxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__90nNs3ucxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directories\n",
        "train_dir = '/kaggle/input/brain-tumor-mri-dataset/Training/'\n",
        "test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing/'\n",
        "\n",
        "# Load training paths and labels\n",
        "train_paths = []\n",
        "train_labels = []\n",
        "for label in os.listdir(train_dir):\n",
        "    for image in os.listdir(train_dir + label):\n",
        "        train_paths.append(train_dir + label + '/' + image)\n",
        "        train_labels.append(label)\n",
        "train_paths, train_labels = shuffle(train_paths, train_labels)\n",
        "\n",
        "# Load testing paths and labels\n",
        "test_paths = []\n",
        "test_labels = []\n",
        "for label in os.listdir(test_dir):\n",
        "    for image in os.listdir(test_dir + label):\n",
        "        test_paths.append(test_dir + label + '/' + image)\n",
        "        test_labels.append(label)\n",
        "test_paths, test_labels = shuffle(test_paths, test_labels)\n",
        "\n",
        "# Image augmentation function\n",
        "def augment_image(image):\n",
        "    image = Image.fromarray(np.uint8(image))\n",
        "    image = ImageEnhance.Brightness(image).enhance(random.uniform(0.8, 1.2))\n",
        "    image = ImageEnhance.Contrast(image).enhance(random.uniform(0.8, 1.2))\n",
        "    image = np.array(image) / 255.0\n",
        "    return image\n",
        "\n",
        "# Function to load images\n",
        "def open_images(paths):\n",
        "    images = []\n",
        "    for path in paths:\n",
        "        image = load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "        image = augment_image(image)\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Get unique labels\n",
        "unique_labels = os.listdir(train_dir)\n",
        "\n",
        "# Label encoding and decoding functions\n",
        "def encode_label(labels):\n",
        "    return np.array([unique_labels.index(x) for x in labels])\n",
        "\n",
        "def decode_label(labels):\n",
        "    return np.array([unique_labels[x] for x in labels])\n",
        "\n",
        "# Data generator function\n",
        "def datagen(paths, labels, batch_size=12, epochs=1):\n",
        "    for _ in range(epochs):\n",
        "        for x in range(0, len(paths), batch_size):\n",
        "            batch_paths = paths[x:x + batch_size]\n",
        "            batch_images = open_images(batch_paths)\n",
        "            batch_labels = labels[x:x + batch_size]\n",
        "            batch_labels = encode_label(batch_labels)\n",
        "            yield batch_images, batch_labels\n",
        "\n",
        "# Load ResNet50 model\n",
        "IMAGE_SIZE = 128\n",
        "base_model = ResNet50(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(unique_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 45\n",
        "steps = int(len(train_paths) / batch_size)\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),\n",
        "                    epochs=epochs, steps_per_epoch=steps)\n",
        "\n",
        "# Federated Learning Based Transfer Learning\n",
        "NUM_CLIENTS = 10\n",
        "NUM_ROUNDS = 3\n",
        "\n",
        "# Define the clients and their data\n",
        "clients = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    client_data = train_paths[i * (len(train_paths) // NUM_CLIENTS):(i + 1) * (len(train_paths) // NUM_CLIENTS)]\n",
        "    client_labels = train_labels[i * (len(train_labels) // NUM_CLIENTS):(i + 1) * (len(train_labels) // NUM_CLIENTS)]\n",
        "    clients.append((client_data, client_labels))\n",
        "\n",
        "# Federated learning loop\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "    # Select clients\n",
        "    selected_client_indices = np.random.choice(len(clients), size=int(NUM_CLIENTS * 0.5), replace=False)\n",
        "    selected_clients = [clients[i] for i in selected_client_indices]\n",
        "\n",
        "    # Transmit the global model to the selected clients\n",
        "    for client in selected_clients:\n",
        "        client_model = tf.keras.models.clone_model(model)\n",
        "        client_model.set_weights(model.get_weights())\n",
        "\n",
        "        # Compile the client model\n",
        "        client_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                             loss='sparse_categorical_crossentropy',\n",
        "                             metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "        steps_per_epoch = int(len(client[0]) / 20)\n",
        "\n",
        "        # Train locally\n",
        "        client_model.fit(datagen(client[0], client[1], batch_size=batch_size, epochs=5), epochs=5, steps_per_epoch=steps_per_epoch)\n",
        "\n",
        "        # Aggregate the model\n",
        "        new_weights = []\n",
        "        for layer_index in range(len(model.get_weights())):\n",
        "            new_layer_weights = np.mean([client_model.get_weights()[layer_index], model.get_weights()[layer_index]], axis=0)\n",
        "            new_weights.append(new_layer_weights)\n",
        "        model.set_weights(new_weights)\n",
        "\n",
        "# Evaluate on test data\n",
        "batch_size = 32\n",
        "steps = int(len(test_paths) / batch_size)\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps):\n",
        "    pred = model.predict(x)\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "    y_pred.extend(decode_label(pred))\n",
        "    y_true.extend(decode_label(y))\n",
        "\n",
        "# Save the model\n",
        "model.save('my_model.h5')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T12:37:31.515977Z",
          "iopub.execute_input": "2024-09-25T12:37:31.516343Z",
          "iopub.status.idle": "2024-09-25T13:30:52.895899Z",
          "shell.execute_reply.started": "2024-09-25T12:37:31.516314Z",
          "shell.execute_reply": "2024-09-25T13:30:52.8948Z"
        },
        "trusted": true,
        "id": "DUiNs_vgucxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T13:30:52.897921Z",
          "iopub.execute_input": "2024-09-25T13:30:52.898244Z",
          "iopub.status.idle": "2024-09-25T13:30:52.929558Z",
          "shell.execute_reply.started": "2024-09-25T13:30:52.898215Z",
          "shell.execute_reply": "2024-09-25T13:30:52.928755Z"
        },
        "trusted": true,
        "id": "rJ98z6XSucxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Define the font size\n",
        "font_size = 20\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=unique_labels, yticklabels=unique_labels, annot_kws={\"fontsize\": font_size}, cbar=False)\n",
        "plt.xlabel(\"Predicted Label\", fontsize=font_size)\n",
        "plt.ylabel(\"True Label\", fontsize=font_size)\n",
        "plt.xticks(fontsize=font_size)\n",
        "plt.yticks(fontsize=font_size, rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T13:30:52.930683Z",
          "iopub.execute_input": "2024-09-25T13:30:52.93096Z",
          "iopub.status.idle": "2024-09-25T13:30:53.186483Z",
          "shell.execute_reply.started": "2024-09-25T13:30:52.930934Z",
          "shell.execute_reply": "2024-09-25T13:30:53.185745Z"
        },
        "trusted": true,
        "id": "S73JOdlFucxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-Ut6ItpucxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Lk07eeVucxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6_em1QuucxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9TToe-VZucxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aThCYEb8ucxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}